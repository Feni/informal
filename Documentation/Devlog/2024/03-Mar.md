# March 2024

## Mar 23
I've accidentlaly designed an OOP language. Let me explain.
Informal was always built on pure, immutable, functional core. I added objects as a way of managing isolated state and concurrency, but it was relegated as a special-case. But there was always a gap - at some level, you need to build the immutable functional things out of something. You need to translate that into hardware. Into real-world systems, and there was a semantic mismatch there which was bugging me.
Objects are that mapping to real world processes. Objects and functions are both equally powerful concepts. You can represent either one in terms of the other. But at the end of the day, the underlying hardware executing the program operates as a process - a series of instructions operating on memory. If we make objects the foundation, then we build up all other notions on top of it - including functions. 
What I'm defining as an object here is not the class-based Java style of objects - that OOP is concerned with modelling a domain as a class hierarchy - which has a lot of problems I won't get into here.
No - objects and classes in Informal are from the smalltalk family of objects - which are isolated, actor-like processes which encapsulate state and interact through messages.
Crucially, messages are *not* the same as functions. A message is a one way interaction between two objects. It is that arrow connecting one to the other. It doesn't require the object to respond to it. With one way messages, you can build higher-level types of communication patterns - like 1:1 call-response, 1:N broadcasts or iterator patterns, or other coordination systems. It is a more primitive unit of control-flow which can encode all types of behavior the system requires - from conditions, functions, exceptions, and concurrent actors.
Objects in Informal represent behavior or protocols between systems. It is that fabric in between things where all of the emergent properties happen - and also the layer which is typically invisible and unstructured. Informal formalizes these behaviors in a way you can analyze.
Each function will have a "process" method - serving as its main point of entry.
Within the process method, it can receive messages and specify what type it expects. It can send out other messages, and then expect particular replies. At the end of the process, it can terminate or continue by recursing back.
What this encodes is the interaction between system. If a process *expects* an "ack" message, but the other part of the system doesn't send it or sends it out of order, we can staticaly catch that. If there are two concurrent systems which are interacting with the system, it can enforce what the valid series of operations are at each state and catch race conditions at compile time. The system must be deterministically well behaved in all cases to fit the protocol.
This informally encodes a kind of linear type - encoding a series of operations in a particular order. That can catch issues like acquiring memory, without releasing it, or use-after-free, and more importantly - you can use it to model the expected processes in your own programs.
This gives us a way to fundamentally model mutable state, IO and many other pesky problems. These are the effects.
With objects as the basis, many other parts of the compiler fall into place. For example, managing scope, type-checking, "assignment" and more. You can represent all of those things completely in-language as just compile-time evaluated code. Giving you a very clean system that is homoiconic and self-represented.
Sometimes adding things makes the problem dramtically simpler across a broad spectrum of problems. That usually indicates it is the right design choice. Total minimalism is not the design principle - there's a difference between simplicity and minimalism. You can build hyper minimal systems out of very few primitives, but it is a mind-bending, convoluted puzzle to put together working systems out of that. Add concepts which simplify, and remove that which adds complexity.


## March 25
### Symbol resolution data structure
The standard data structures used to maintain scopes are either a linked hash-map with parent pointers to previous scopes, trees, immutable maps like hash array mapped tries. 
Here's another option optimized for fast lookup, minimal memory and single pass resolution of forward references.
At any depth in the code, the only symbols that are relevant are those in its parent hierarchy. And once a scope is closed, its symbols are no longer needed anywhere. That is analogous to how stack frames work - you open up a frame for a depth, and then free it when exiting a scope. Ofcourse, linearly searching through that stack isn't going to be fast enough, but we can use the same idea with other underlying structures as long as we know which elements belong in the current depth.
For fast resolution, you need to be able to lookup a key without scaling by depth. And we need to minimize the cost of opening up small scopes (like conditions) near the bottom of the AST. 
We can do this by putting all of the keys in a single hashmap with a bitset per depth indicating if the key is present at that depth. When you open up a new scope, all you need to allocate is that small bitset. When you lookup a symbol, it's just a hash table lookup. When you declare a new symbol, set a 1 at the key-index in the bitset for your depth, lookup the location and if it's already populated (i.e. shadowing or collisions), push off the previous value into a linked list. Thus, lookups always get you the latest value, but you have the values from the parent scopes available in the linked list. When a scope is closed, we don't need to cleanup *all* entries, just the ones which were populated before - you can easily find that by AND-ing the bitset for the current depth with bitsets from prior levels (OR-ing them all together to collapse duplicates). You iterate to just those entries and pop the linked list to restore the previous value. Et voila! You're back to the hash map you had before this scope.
Now how do we resolve forward resolution? When a local variable references something from a higher scope you haven't seen yet, add it into the hashmap and set a bit to indicate it's a missing key for depth N. Now, when something declares that value at a later point, you can check if it was defined at depth N or lower - in which case, it's the reference we were looking for. Traverse through those link links and pop all of the unresolved references. Allowing references to resolve linearly as you come across declarations.
Couple of optimizations:
This algorithm works with various types of underlying storage. You can apply it to trees, radix/prefix tries, etc. as well. 
When there are tens of thousands of references in a sourcecode, allocating the bitset per layer can get expensive if we need 1KB of bits. You can reduce this in two ways. 
One - most symbols come from external hashmaps - populate that into a separate static hashmap - you'll then need two lookups, but it shrinks the active symbol table dramatically.
Two - Recognize that the bitsets for a layer will be sparse and define it hierarchically. I initially planned to split up the hashmap itself into a hierarchy, but you don't actually need to do that. Bitset hierarchies are something I'm finding broadly useful. When you enter a new scope, just allocate a single 64 bit value indicating the 'slices' where this scope has values. When a new value is added, set a 1 in that bit, then add the second layer to the bitset indicating the sub-slice - and so on until you can resolve to the exact node you want. These layers are ordered by the popcount order - so you have to shift it on insertion to maintain that order - but indexing into a certain array element is just mask + popcount + index offset. This remains compact and handles the sparsity well. You only need a small stack to maintain this. Resizing the hashmap gets complicated but is certainly do-able. 

### Recursive descent with shunting yard / shift reduce expression parser
After trying out many other kinds of parsing techniques, I've come back to where I initially started but with a much clearer understanding of the problem space. What I want is just a plain recursive descent to handle higher-level structures in code, with a shunting-yard or shift/reduce style parser for the expressions. It maintains an operator stack, and uses a parser-driven lexer to minimize intermediate storage. You branch into recursive sub-expressions based on the top-level token type you see - i.e. I see a number, what's the valid actions for each kind of token that can appear after it. You just do a lookup by token type to determine whether it should output it, pop elements from the stack, whether it indicates an error, or whether that token type has custom code handling. Since the number of tokens are small, you can encode each of these 'maps' as just bitsets - fitting a lot of parsing logic into a few 64 bit values. Similar 'maps' can also indicate which elements from the operator stack to flush (encoding the result of precedence + associtivity or other rules). So most of this is table driven, but a different kind of table driven parser than the generalized parser I was designing before. By wrapping this whole approach in 'recursive' descent, you maintain the readability, flexibility and error handling of just raw code but avoid a lot of the pathlogical recursive cases. The output of this is just a postorder bytecode stack, which combined with the above symbol resolution gives you something that's almost ready to execute.

## March 27
### Informal Type Checker
Sufficiently advanced type systems are turning complete systems in their own rights. They have the propensity to become quite complex and unweildy over time with all of the rules they encode. Rather than an ad-hoc implementation, we define the informal type system as its own mini-language evaluated through a specilized bytecode interpreter. This architecture gives us the expressive power to model advanced type systems, including dependent and linear types. It builds off of the bytecode other parts of the compiler already generate, but evaluating it at a symbolic type-level.

Take for example, a block of code like:
a = 3 + 5
b = foo(x)
y = if: 
    x > 0: a
    else: b

Without knowing the actual runtime values, we can still glean a lot of useful information from this.
Constant expressions like z are evaluated at compile time at earlier stages of compilation, giving you a constant value to work with.
An abstract function call like foo(x) can't be evaluated at compile time, but we can infer that the return value would atleast meet the type signature of the function. If you know more information about x (for example, whether it's positive), you can partially evaluate foo and use a more refined type.
Conditions and loops are where things get interesting. The condition represents an exhaustive universe of possibilities. Either x > 0 and thus y = a, or x <= 0 and y = b. Rather than an abstract Integer x, we now think of x's possible values in two slices - together they still cover the full Integer space, but these sub-types allow us to more precisely define what else is implied. Rather than just representing y's type as just a | b, we need to consider the entire state. This is where the bytecode style of evaluation become useful. You define these intermediate states just as intermediate variables. Define dependencies between these variables - cuts/subsets of a parent Type (Like x: Int being divided into x_tmp0: Positive, x_tmp1: Zero | Negative), or implications (if x_tmp0 is shown to be true elsewhere, then we know y must be a as well. Or the inverse, if y is checked as being equal to a elsewhere, then within that section of code we can assume x is x_tmp0).
There's a lot of overlap with boolean satisfiability and integer linear programming. The bytecode evaluator basically works by evaluating the operations in the abstract, giving you either a precise answer or a simplified expression or a more complex expression or a conflict. If a variable implies that x is Int and then something later on defines another possibility on top, like x is Int and x > 0, you can simplify that down. The statement x is Int is equivalent of saying x > MIN_INT and x < MAX_INT. But with the new statement, you have x1 = x > 0 AND x < MAX_INT, which constrains the possibilities down further. Now if you have some x3 which refine the upper bound of that further, you can collapse the operations and constrain the type even further.
"and" expressions are thus very useful for narrowing types down further and further, but "or" bifrucates the potential possibilities. We manage that by implicitly breaking that down into two sub-variables - this is similar to the condition, but not quite. When you had the type Int, with two sub-types below it - when the top-level check for whether x is an Int is false, then neither of the sub-types would be true as well. But if it's true and we wnat to know more about it, when you check one branch you know automatically the result of the other branch since they're non-overlapping. Each branch implies that the previous branch was false in addition to implying its own condition was true. In the general case of boolean expressions that's not the case - you have to split up the analysis into three parts - you can combine the boolean expression into a top-level Union - if that is false, then none of the child-expressions will be true. Inside, it's split into 3 slices - A, B and the intersection of A and B. If we know the parent value is true, then checking any two will imply the third's value. This works for boolean expressions and we can evaluate a lot of these possibilities simultaneously with some clever low-level programming. A similar approach can be fruitful for integers by encoding information about ranges and doing abstract operations on the range of potential values.
What this ultimately gives us is an abstract evaluator which can run at compile time in a fairly efficient manner through a bytecode evaluator which derives abstract facts about all variables in the system. This is valuable both from a type-checking prespective, but also from an optimization prespective for later stages of the compiler. There are a lot of potentially combinatorial factors in this algorithms like this. What makes it tractable is treating it as modular, isolated checks rather than trying to check global facts.

## March 28
### Automatic memory management with generational stack references
In addition to the type-checking thoughts above, I've been thinking about memory allocation. The plan previously was to use a GC, but with the process/linear types and an efficient way of checking it, an ownership/borrowing model becomes viable. These are still rough thoughts. A lot of details are in flux, so consider this a work in progress.
What is garbage? It's memory we don't need anymore, or more specifically memory we can't really reference anymore because we've thrown away all "live" references to it. At its core, liveness comes from whether we reference a some memory directly or indirectly in the active execution stack. Liveness originates from the stack and then transitively infects heap variables. Why do we have the stack and heap split? Because it's beneficial to keep the stack "small" and of a known/fixed size. You don't want to stuff a massive file you read from disk onto the stack because you don't know how big it's going to be and it makes it harder to index other stack variables just by stack offsets if you don't know the fixed size of things. So we throw them somewhere else - the heap. But whenever possible, the stack is the best place to allocate things - it automatically manages its own memory in a trivial way - it's super easy to allocate and deallocate things in a bump-allocator style. And it'll be hot in cache. C has a lot of calling conventions on how you should save registers onto the stack before calling a function. Informal prefers passing variables through registers over the stack whenever possible and tries to do register allocation to make that possible. Additionally, when functions return data, you don't want to just repeatedly pass that variable up through multiple layers by copying it over and over again through the stack - ideally, we statically determine where it should end up and then put that there. If you're calling a function deep within the call stack and if it's called from multiple places it can't know at which offset that result goes - so the return destination is also passed in as a parameter (either in register or at a stack offset). What this ultimately lets us do is put each variable in the correct location which specify how long that variable is "live". It's reserved space at the oldest location it will be needed and just directly assigned there.
This leads us to the key heuristic of our memory management approach. Stack objects or more broadly, the stack frames dictate the lifetime of data. When you pop an element from the stack, that variable and any transitive dependencies are now garbage. You could go and mark that as garbage and have some separate process come through and collect it (that process can take care of tracing other nested references) - but then you're adding writes elsewhere in memory quite frequently. A better way to manage this would be at a higher level - we use a segmented stack approach which grows over time. When you exit from a stack segment, every variable that was ever allocated in that stack segment is now garbage. If each segment had the notion of a generation, then it has now entered a new generation and the old generation refs from that stack are all garbage now. You could store the generation ID on each pointer or within book-keeping sections of the heap, but we can do better. The heap is per-process and allocated linearly. So whenever a stack segment or generation is created, if we record the current heap top, then we know that any variable after that was created in that generation and when that generation passes all of that is garbage. You can collect it by just resetting the heap top back to where it was and reusing that space. To prevent the problem of generations getting incremented repeatedly when you enter and exit a stack segment you add in a heuristic that it only counts as a generation when you hit the top of the stack and then hit the bottom - i.e. the segment goes through a full fill/clear lifecycle.
There's an important edge case here with our previously described optimization of allocating return values where they should ultimately end up. That becomes an exception to the rule that the heap is in linear lifetime order. And it's not possible for us to put that data in older regions since it's completely packed. The solution here is for each stack segment to also define a secondary stack-specific heap space. This is a linked list of smaller heap regions where you can come back later and add elements. Just like the main heap, you can also collect this entire dynamic heap in one batch whenever the stack generation passes.
The core axiom to make this all work is perfect lifetime resolution. Which I think is possible if everything is immutable, but I do need to reason through the mutable case some more. If the lifetime resolution holds, this ends up as a very efficient mechanism for managing memory in bulk with very minimal overhead and just generally has a lot of great characteristics.

